{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konfigurasi stand alone (nilai 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop [--config confdir] [--loglevel loglevel] COMMAND\n",
      "where COMMAND is one of:\n",
      "  fs                   run a generic filesystem user client\n",
      "  version              print the version\n",
      "  jar <jar>            run a jar file\n",
      "                       note: please use \"yarn jar\" to launch\n",
      "                             YARN applications, not this command.\n",
      "  checknative [-a|-h]  check native hadoop and compression libraries availability\n",
      "  distcp <srcurl> <desturl> copy file or directories recursively\n",
      "  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive\n",
      "  classpath            prints the class path needed to get the\n",
      "                       Hadoop jar and the required libraries\n",
      "  credential           interact with credential providers\n",
      "  key                  manage keys via the KeyProvider\n",
      "  daemonlog            get/set the log level for each daemon\n",
      " or\n",
      "  CLASSNAME            run the class named CLASSNAME\n",
      "\n",
      "Most commands print help when invoked w/o parameters.\n"
     ]
    }
   ],
   "source": [
    "!C:\\Users\\wirar\\hadoop\\bin\\hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ~/hadoop/hadoop/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ~/hadoop/hadoop/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/hadoop/hadoop/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp C:\\Users\\wirar\\hadoop\\etc\\hadoop\\*.xml C:\\Users\\wirar\\hadoop\\etc\\hadoop\\input\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ~/hadoop/hadoop/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/24 13:19:55 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "23/10/24 13:19:55 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "23/10/24 13:19:56 INFO input.FileInputFormat: Total input files to process : 1\n",
      "23/10/24 13:19:56 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "23/10/24 13:19:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local815001253_0001\n",
      "23/10/24 13:19:56 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "23/10/24 13:19:56 INFO mapreduce.Job: Running job: job_local815001253_0001\n",
      "23/10/24 13:19:56 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "23/10/24 13:19:56 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/10/24 13:19:56 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/10/24 13:19:56 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/10/24 13:19:56 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "23/10/24 13:19:56 INFO mapred.LocalJobRunner: Starting task: attempt_local815001253_0001_m_000000_0\n",
      "23/10/24 13:19:56 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/10/24 13:19:56 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/10/24 13:19:56 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "23/10/24 13:19:56 INFO mapred.Task:  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@4cabe6b8\n",
      "23/10/24 13:19:56 INFO mapred.MapTask: Processing split: file:/C:/Users/wirar/hadoop/input/pg20417.txt:0+661807\n",
      "23/10/24 13:19:56 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "23/10/24 13:19:56 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "23/10/24 13:19:56 INFO mapred.MapTask: soft limit at 83886080\n",
      "23/10/24 13:19:56 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "23/10/24 13:19:56 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "23/10/24 13:19:56 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "23/10/24 13:19:56 INFO mapred.LocalJobRunner: \n",
      "23/10/24 13:19:56 INFO mapred.MapTask: Starting flush of map output\n",
      "23/10/24 13:19:56 INFO mapred.MapTask: Spilling map output\n",
      "23/10/24 13:19:56 INFO mapred.MapTask: bufstart = 0; bufend = 21; bufvoid = 104857600\n",
      "23/10/24 13:19:56 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
      "23/10/24 13:19:56 INFO mapred.MapTask: Finished spill 0\n",
      "23/10/24 13:19:56 INFO mapred.Task: Task:attempt_local815001253_0001_m_000000_0 is done. And is in the process of committing\n",
      "23/10/24 13:19:56 INFO mapred.LocalJobRunner: map\n",
      "23/10/24 13:19:56 INFO mapred.Task: Task 'attempt_local815001253_0001_m_000000_0' done.\n",
      "23/10/24 13:19:56 INFO mapred.Task: Final Counters for attempt_local815001253_0001_m_000000_0: Counters: 18\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=965288\n",
      "\t\tFILE: Number of bytes written=800407\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12760\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=21\n",
      "\t\tMap output materialized bytes=29\n",
      "\t\tInput split bytes=110\n",
      "\t\tCombine input records=1\n",
      "\t\tCombine output records=1\n",
      "\t\tSpilled Records=1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=264241152\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=661807\n",
      "23/10/24 13:19:56 INFO mapred.LocalJobRunner: Finishing task: attempt_local815001253_0001_m_000000_0\n",
      "23/10/24 13:19:56 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "23/10/24 13:19:56 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "23/10/24 13:19:56 INFO mapred.LocalJobRunner: Starting task: attempt_local815001253_0001_r_000000_0\n",
      "23/10/24 13:19:56 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/10/24 13:19:56 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/10/24 13:19:56 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "23/10/24 13:19:56 INFO mapred.Task:  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@60a88c2c\n",
      "23/10/24 13:19:56 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1cc19d46\n",
      "23/10/24 13:19:56 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=375809632, maxSingleShuffleLimit=93952408, mergeThreshold=248034368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "23/10/24 13:19:56 INFO reduce.EventFetcher: attempt_local815001253_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "23/10/24 13:19:56 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local815001253_0001_m_000000_0 decomp: 25 len: 29 to MEMORY\n",
      "23/10/24 13:19:56 INFO reduce.InMemoryMapOutput: Read 25 bytes from map-output for attempt_local815001253_0001_m_000000_0\n",
      "23/10/24 13:19:56 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 25, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->25\n",
      "23/10/24 13:19:56 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "23/10/24 13:19:56 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "23/10/24 13:19:56 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "23/10/24 13:19:56 INFO mapred.Merger: Merging 1 sorted segments\n",
      "23/10/24 13:19:56 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 10 bytes\n",
      "23/10/24 13:19:56 INFO reduce.MergeManagerImpl: Merged 1 segments, 25 bytes to disk to satisfy reduce memory limit\n",
      "23/10/24 13:19:56 INFO reduce.MergeManagerImpl: Merging 1 files, 29 bytes from disk\n",
      "23/10/24 13:19:56 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "23/10/24 13:19:56 INFO mapred.Merger: Merging 1 sorted segments\n",
      "23/10/24 13:19:56 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 10 bytes\n",
      "23/10/24 13:19:56 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "23/10/24 13:19:56 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "23/10/24 13:19:56 INFO mapred.Task: Task:attempt_local815001253_0001_r_000000_0 is done. And is in the process of committing\n",
      "23/10/24 13:19:56 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "23/10/24 13:19:56 INFO mapred.Task: Task attempt_local815001253_0001_r_000000_0 is allowed to commit now\n",
      "23/10/24 13:19:56 INFO output.FileOutputCommitter: Saved output of task 'attempt_local815001253_0001_r_000000_0' to file:/C:/Users/wirar/OneDrive/Documents/GitHub/bigdata/grep-temp-1419684470/_temporary/0/task_local815001253_0001_r_000000\n",
      "23/10/24 13:19:56 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "23/10/24 13:19:56 INFO mapred.Task: Task 'attempt_local815001253_0001_r_000000_0' done.\n",
      "23/10/24 13:19:56 INFO mapred.Task: Final Counters for attempt_local815001253_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=965378\n",
      "\t\tFILE: Number of bytes written=800563\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=29\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=1\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=2\n",
      "\t\tTotal committed heap usage (bytes)=264241152\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=127\n",
      "23/10/24 13:19:56 INFO mapred.LocalJobRunner: Finishing task: attempt_local815001253_0001_r_000000_0\n",
      "23/10/24 13:19:56 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "23/10/24 13:19:57 INFO mapreduce.Job: Job job_local815001253_0001 running in uber mode : false\n",
      "23/10/24 13:19:57 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "23/10/24 13:19:57 INFO mapreduce.Job: Job job_local815001253_0001 completed successfully\n",
      "23/10/24 13:19:57 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1930666\n",
      "\t\tFILE: Number of bytes written=1600970\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12760\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=21\n",
      "\t\tMap output materialized bytes=29\n",
      "\t\tInput split bytes=110\n",
      "\t\tCombine input records=1\n",
      "\t\tCombine output records=1\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=29\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=2\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=2\n",
      "\t\tTotal committed heap usage (bytes)=528482304\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=661807\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=127\n",
      "23/10/24 13:19:57 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "23/10/24 13:19:57 INFO input.FileInputFormat: Total input files to process : 1\n",
      "23/10/24 13:19:57 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "23/10/24 13:19:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local34040442_0002\n",
      "23/10/24 13:19:57 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "23/10/24 13:19:57 INFO mapreduce.Job: Running job: job_local34040442_0002\n",
      "23/10/24 13:19:57 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "23/10/24 13:19:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/10/24 13:19:57 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/10/24 13:19:57 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/10/24 13:19:57 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "23/10/24 13:19:57 INFO mapred.LocalJobRunner: Starting task: attempt_local34040442_0002_m_000000_0\n",
      "23/10/24 13:19:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/10/24 13:19:57 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/10/24 13:19:57 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "23/10/24 13:19:57 INFO mapred.Task:  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@29bb8ec1\n",
      "23/10/24 13:19:57 INFO mapred.MapTask: Processing split: file:/C:/Users/wirar/OneDrive/Documents/GitHub/bigdata/grep-temp-1419684470/part-r-00000:0+115\n",
      "23/10/24 13:19:57 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "23/10/24 13:19:57 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "23/10/24 13:19:57 INFO mapred.MapTask: soft limit at 83886080\n",
      "23/10/24 13:19:57 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "23/10/24 13:19:57 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "23/10/24 13:19:57 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "23/10/24 13:19:57 INFO mapred.LocalJobRunner: \n",
      "23/10/24 13:19:57 INFO mapred.MapTask: Starting flush of map output\n",
      "23/10/24 13:19:57 INFO mapred.MapTask: Spilling map output\n",
      "23/10/24 13:19:57 INFO mapred.MapTask: bufstart = 0; bufend = 21; bufvoid = 104857600\n",
      "23/10/24 13:19:57 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
      "23/10/24 13:19:57 INFO mapred.MapTask: Finished spill 0\n",
      "23/10/24 13:19:57 INFO mapred.Task: Task:attempt_local34040442_0002_m_000000_0 is done. And is in the process of committing\n",
      "23/10/24 13:19:57 INFO mapred.LocalJobRunner: map\n",
      "23/10/24 13:19:57 INFO mapred.Task: Task 'attempt_local34040442_0002_m_000000_0' done.\n",
      "23/10/24 13:19:57 INFO mapred.Task: Final Counters for attempt_local34040442_0002_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1269026\n",
      "\t\tFILE: Number of bytes written=1597306\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=21\n",
      "\t\tMap output materialized bytes=29\n",
      "\t\tInput split bytes=153\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=264241152\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=127\n",
      "23/10/24 13:19:57 INFO mapred.LocalJobRunner: Finishing task: attempt_local34040442_0002_m_000000_0\n",
      "23/10/24 13:19:57 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "23/10/24 13:19:57 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "23/10/24 13:19:57 INFO mapred.LocalJobRunner: Starting task: attempt_local34040442_0002_r_000000_0\n",
      "23/10/24 13:19:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/10/24 13:19:57 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/10/24 13:19:57 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "23/10/24 13:19:57 INFO mapred.Task:  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@729b3740\n",
      "23/10/24 13:19:57 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@76a4adcb\n",
      "23/10/24 13:19:57 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=375809632, maxSingleShuffleLimit=93952408, mergeThreshold=248034368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "23/10/24 13:19:57 INFO reduce.EventFetcher: attempt_local34040442_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "23/10/24 13:19:57 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local34040442_0002_m_000000_0 decomp: 25 len: 29 to MEMORY\n",
      "23/10/24 13:19:57 INFO reduce.InMemoryMapOutput: Read 25 bytes from map-output for attempt_local34040442_0002_m_000000_0\n",
      "23/10/24 13:19:57 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 25, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->25\n",
      "23/10/24 13:19:57 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "23/10/24 13:19:57 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "23/10/24 13:19:57 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "23/10/24 13:19:57 INFO mapred.Merger: Merging 1 sorted segments\n",
      "23/10/24 13:19:57 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 15 bytes\n",
      "23/10/24 13:19:57 INFO reduce.MergeManagerImpl: Merged 1 segments, 25 bytes to disk to satisfy reduce memory limit\n",
      "23/10/24 13:19:57 INFO reduce.MergeManagerImpl: Merging 1 files, 29 bytes from disk\n",
      "23/10/24 13:19:57 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "23/10/24 13:19:57 INFO mapred.Merger: Merging 1 sorted segments\n",
      "23/10/24 13:19:57 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 15 bytes\n",
      "23/10/24 13:19:57 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "23/10/24 13:19:57 INFO mapred.Task: Task:attempt_local34040442_0002_r_000000_0 is done. And is in the process of committing\n",
      "23/10/24 13:19:57 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "23/10/24 13:19:57 INFO mapred.Task: Task attempt_local34040442_0002_r_000000_0 is allowed to commit now\n",
      "23/10/24 13:19:57 INFO output.FileOutputCommitter: Saved output of task 'attempt_local34040442_0002_r_000000_0' to file:/C:/Users/wirar/hadoop/output/_temporary/0/task_local34040442_0002_r_000000\n",
      "23/10/24 13:19:57 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "23/10/24 13:19:57 INFO mapred.Task: Task 'attempt_local34040442_0002_r_000000_0' done.\n",
      "23/10/24 13:19:57 INFO mapred.Task: Final Counters for attempt_local34040442_0002_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1269116\n",
      "\t\tFILE: Number of bytes written=1597362\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=29\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=1\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=264241152\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=27\n",
      "23/10/24 13:19:57 INFO mapred.LocalJobRunner: Finishing task: attempt_local34040442_0002_r_000000_0\n",
      "23/10/24 13:19:57 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "23/10/24 13:19:58 INFO mapreduce.Job: Job job_local34040442_0002 running in uber mode : false\n",
      "23/10/24 13:19:58 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "23/10/24 13:19:58 INFO mapreduce.Job: Job job_local34040442_0002 completed successfully\n",
      "23/10/24 13:19:58 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2538142\n",
      "\t\tFILE: Number of bytes written=3194668\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=21\n",
      "\t\tMap output materialized bytes=29\n",
      "\t\tInput split bytes=153\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=29\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=2\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=528482304\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=127\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=27\n"
     ]
    }
   ],
   "source": [
    "!C:\\Users\\wirar\\hadoop\\bin\\hadoop jar C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-examples-2.10.2.jar grep C:\\Users\\wirar\\hadoop\\input C:\\Users\\wirar\\hadoop\\output '[a-z]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/hadoop/hadoop/output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampai disini bisa nilai :80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konfigurasi HDFS Server (nilai 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!xed ~/hadoop/hadoop/etc/hadoop/core-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://localhost:9000</value>\n",
    "    </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!xed ~/hadoop/hadoop/etc/hadoop/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<configuration>\n",
    "    <property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>1</value>\n",
    "    </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting using clusterid: CID-530de97b-66de-447f-8a20-49db17b1fecb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/24 13:23:19 INFO namenode.NameNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting NameNode\n",
      "STARTUP_MSG:   host = wirarama/192.168.82.10\n",
      "STARTUP_MSG:   args = [-format]\n",
      "STARTUP_MSG:   version = 2.10.2\n",
      "STARTUP_MSG:   classpath = C:\\Users\\wirar\\hadoop\\etc\\hadoop;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\activation-1.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\apacheds-i18n-2.0.0-M15.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\apacheds-kerberos-codec-2.0.0-M15.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\api-asn1-api-1.0.0-M20.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\api-util-1.0.0-M20.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\asm-3.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\audience-annotations-0.5.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\avro-1.7.7.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-beanutils-1.9.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-cli-1.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-codec-1.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-collections-3.2.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-compress-1.21.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-configuration-1.6.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-digester-1.8.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-io-2.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-lang-2.6.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-lang3-3.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-logging-1.1.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-math3-3.1.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-net-3.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\curator-client-2.13.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\curator-framework-2.13.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\curator-recipes-2.13.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\gson-2.2.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\guava-11.0.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\hadoop-annotations-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\hadoop-auth-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\hamcrest-core-1.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\htrace-core4-4.1.0-incubating.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\httpclient-4.5.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\httpcore-4.4.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jackson-core-asl-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jackson-jaxrs-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jackson-mapper-asl-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jackson-xc-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\java-xmlbuilder-0.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jaxb-api-2.2.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jaxb-impl-2.2.3-1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jcip-annotations-1.0-1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jersey-core-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jersey-json-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jersey-server-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jets3t-0.9.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jettison-1.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jetty-6.1.26.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jetty-sslengine-6.1.26.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jetty-util-6.1.26.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jsch-0.1.55.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\json-smart-1.3.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jsp-api-2.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jsr305-3.0.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\junit-4.13.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\mockito-all-1.8.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\netty-3.10.6.Final.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\nimbus-jose-jwt-7.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\paranamer-2.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\protobuf-java-2.5.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\reload4j-1.2.18.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\servlet-api-2.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\slf4j-api-1.7.36.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\slf4j-reload4j-1.7.36.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\snappy-java-1.0.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\spotbugs-annotations-3.1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\stax-api-1.0-2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\stax2-api-4.2.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\woodstox-core-5.3.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\xmlenc-0.52.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\zookeeper-3.4.14.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\hadoop-common-2.10.2-tests.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\hadoop-common-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\hadoop-nfs-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\asm-3.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\commons-cli-1.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\commons-codec-1.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\commons-daemon-1.0.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\commons-io-2.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\commons-lang-2.6.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\commons-logging-1.1.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\guava-11.0.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\hadoop-hdfs-client-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\htrace-core4-4.1.0-incubating.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jackson-annotations-2.9.10.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jackson-core-2.9.10.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jackson-core-asl-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jackson-databind-2.9.10.7.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jackson-mapper-asl-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jersey-core-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jersey-server-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jetty-6.1.26.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jetty-util-6.1.26.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jsr305-3.0.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\leveldbjni-all-1.8.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\netty-3.10.6.Final.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\netty-all-4.1.50.Final.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\okhttp-2.7.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\okio-1.6.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\protobuf-java-2.5.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\reload4j-1.2.18.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\servlet-api-2.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\xercesImpl-2.12.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\xml-apis-1.4.01.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\xmlenc-0.52.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-2.10.2-tests.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-client-2.10.2-tests.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-client-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-native-client-2.10.2-tests.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-native-client-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-nfs-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-rbf-2.10.2-tests.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-rbf-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\activation-1.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\aopalliance-1.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\apacheds-i18n-2.0.0-M15.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\apacheds-kerberos-codec-2.0.0-M15.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\api-asn1-api-1.0.0-M20.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\api-util-1.0.0-M20.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\asm-3.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\audience-annotations-0.5.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\avro-1.7.7.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-beanutils-1.9.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-cli-1.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-codec-1.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-collections-3.2.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-compress-1.21.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-configuration-1.6.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-digester-1.8.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-io-2.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-lang-2.6.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-lang3-3.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-logging-1.1.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-math3-3.1.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-net-3.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\curator-client-2.13.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\curator-framework-2.13.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\curator-recipes-2.13.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\ehcache-3.3.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\fst-2.50.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\geronimo-jcache_1.0_spec-1.0-alpha-1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\gson-2.2.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\guava-11.0.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\guice-3.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\guice-servlet-3.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\HikariCP-java7-2.4.12.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\htrace-core4-4.1.0-incubating.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\httpclient-4.5.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\httpcore-4.4.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jackson-core-asl-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jackson-jaxrs-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jackson-mapper-asl-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jackson-xc-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\java-util-1.9.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\java-xmlbuilder-0.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\javax.inject-1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jaxb-api-2.2.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jaxb-impl-2.2.3-1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jcip-annotations-1.0-1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jersey-client-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jersey-core-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jersey-guice-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jersey-json-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jersey-server-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jets3t-0.9.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jettison-1.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jetty-6.1.26.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jetty-sslengine-6.1.26.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jetty-util-6.1.26.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jsch-0.1.55.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\json-io-2.5.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\json-smart-1.3.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jsp-api-2.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jsr305-3.0.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\leveldbjni-all-1.8.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\metrics-core-3.0.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\mssql-jdbc-6.2.1.jre7.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\netty-3.10.6.Final.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\nimbus-jose-jwt-7.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\paranamer-2.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\protobuf-java-2.5.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\reload4j-1.2.18.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\servlet-api-2.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\snappy-java-1.0.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\spotbugs-annotations-3.1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\stax-api-1.0-2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\stax2-api-4.2.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\woodstox-core-5.3.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\xmlenc-0.52.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\zookeeper-3.4.14.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-api-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-applications-distributedshell-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-applications-unmanaged-am-launcher-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-client-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-common-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-registry-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-applicationhistoryservice-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-common-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-nodemanager-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-resourcemanager-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-router-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-sharedcachemanager-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-tests-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-timeline-pluginstorage-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-web-proxy-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\aopalliance-1.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\asm-3.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\avro-1.7.7.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\commons-compress-1.21.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\commons-io-2.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\guice-3.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\guice-servlet-3.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\hadoop-annotations-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\hamcrest-core-1.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\jackson-core-asl-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\jackson-mapper-asl-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\javax.inject-1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\jersey-core-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\jersey-guice-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\jersey-server-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\junit-4.13.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\leveldbjni-all-1.8.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\netty-3.10.6.Final.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\paranamer-2.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\protobuf-java-2.5.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\reload4j-1.2.18.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\snappy-java-1.0.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-client-app-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-client-common-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-client-core-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-client-hs-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-client-hs-plugins-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-client-jobclient-2.10.2-tests.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-client-jobclient-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-client-shuffle-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-examples-2.10.2.jar\n",
      "STARTUP_MSG:   build = Unknown -r 965fd380006fa78b2315668fbc7eb432e1d8200f; compiled by 'ubuntu' on 2022-05-24T22:35Z\n",
      "STARTUP_MSG:   java = 17.0.9\n",
      "************************************************************/\n",
      "23/10/24 13:23:19 INFO namenode.NameNode: createNameNode [-format]\n",
      "23/10/24 13:23:19 INFO namenode.FSEditLog: Edit logging is async:true\n",
      "23/10/24 13:23:19 INFO namenode.FSNamesystem: KeyProvider: null\n",
      "23/10/24 13:23:19 INFO namenode.FSNamesystem: fsLock is fair: true\n",
      "23/10/24 13:23:19 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
      "23/10/24 13:23:19 INFO namenode.FSNamesystem: fsOwner             = wirar (auth:SIMPLE)\n",
      "23/10/24 13:23:19 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
      "23/10/24 13:23:19 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
      "23/10/24 13:23:19 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "23/10/24 13:23:19 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "23/10/24 13:23:19 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
      "23/10/24 13:23:19 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "23/10/24 13:23:19 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "23/10/24 13:23:19 INFO blockmanagement.BlockManager: The block deletion will start around 2023 Oct 24 13:23:19\n",
      "23/10/24 13:23:19 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "23/10/24 13:23:19 INFO util.GSet: VM type       = 64-bit\n",
      "23/10/24 13:23:19 INFO util.GSet: 2.0% max memory 1000 MB = 20 MB\n",
      "23/10/24 13:23:19 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n",
      "23/10/24 13:23:19 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\n",
      "23/10/24 13:23:19 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS\n",
      "23/10/24 13:23:19 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\n",
      "23/10/24 13:23:19 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
      "23/10/24 13:23:19 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
      "23/10/24 13:23:19 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
      "23/10/24 13:23:19 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
      "23/10/24 13:23:19 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "23/10/24 13:23:19 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "23/10/24 13:23:19 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "23/10/24 13:23:19 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\n",
      "23/10/24 13:23:19 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "23/10/24 13:23:19 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "23/10/24 13:23:19 INFO namenode.FSNamesystem: Append Enabled: true\n",
      "23/10/24 13:23:20 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215\n",
      "23/10/24 13:23:20 INFO util.GSet: Computing capacity for map INodeMap\n",
      "23/10/24 13:23:20 INFO util.GSet: VM type       = 64-bit\n",
      "23/10/24 13:23:20 INFO util.GSet: 1.0% max memory 1000 MB = 10 MB\n",
      "23/10/24 13:23:20 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
      "23/10/24 13:23:20 INFO namenode.FSDirectory: ACLs enabled? false\n",
      "23/10/24 13:23:20 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "23/10/24 13:23:20 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
      "23/10/24 13:23:20 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false\n",
      "23/10/24 13:23:20 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "23/10/24 13:23:20 INFO util.GSet: VM type       = 64-bit\n",
      "23/10/24 13:23:20 INFO util.GSet: 0.25% max memory 1000 MB = 2.5 MB\n",
      "23/10/24 13:23:20 INFO util.GSet: capacity      = 2^18 = 262144 entries\n",
      "23/10/24 13:23:20 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "23/10/24 13:23:20 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "23/10/24 13:23:20 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "23/10/24 13:23:20 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "23/10/24 13:23:20 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "23/10/24 13:23:20 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "23/10/24 13:23:20 INFO util.GSet: VM type       = 64-bit\n",
      "23/10/24 13:23:20 INFO util.GSet: 0.029999999329447746% max memory 1000 MB = 307.2 KB\n",
      "23/10/24 13:23:20 INFO util.GSet: capacity      = 2^15 = 32768 entries\n",
      "23/10/24 13:23:20 INFO namenode.FSImage: Allocated new BlockPoolId: BP-987665854-192.168.82.10-1698125000052\n",
      "23/10/24 13:23:20 INFO common.Storage: Storage directory \\tmp\\hadoop-wirar\\dfs\\name has been successfully formatted.\n",
      "23/10/24 13:23:20 INFO namenode.FSImageFormatProtobuf: Saving image file \\tmp\\hadoop-wirar\\dfs\\name\\current\\fsimage.ckpt_0000000000000000000 using no compression\n",
      "23/10/24 13:23:20 INFO namenode.FSImageFormatProtobuf: Image file \\tmp\\hadoop-wirar\\dfs\\name\\current\\fsimage.ckpt_0000000000000000000 of size 324 bytes saved in 0 seconds .\n",
      "23/10/24 13:23:20 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
      "23/10/24 13:23:20 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid = 0 when meet shutdown.\n",
      "23/10/24 13:23:20 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down NameNode at wirarama/192.168.82.10\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "!C:\\Users\\wirar\\hadoop\\bin\\hdfs namenode -format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!C:\\Users\\wirar\\hadoop\\sbin\\start-dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:50070/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/Users/wiraramawedashwara/hadoop/hadoop/share/hadoop/common/lib/hadoop-auth-2.10.2.jar) to method sun.security.krb5.Config.getInstance()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/11/09 06:51:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/Users/wiraramawedashwara/hadoop/hadoop/share/hadoop/common/lib/hadoop-auth-2.10.2.jar) to method sun.security.krb5.Config.getInstance()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/11/09 06:51:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /user/wiraramawedashwara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/Users/wiraramawedashwara/hadoop/hadoop/share/hadoop/common/lib/hadoop-auth-2.10.2.jar) to method sun.security.krb5.Config.getInstance()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/11/09 06:54:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/wiraramawedashwara/httpdata/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/hadoop/bin/hdfs dfs -put ~/hadoop/hadoop/etc/hadoop input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/hadoop/bin/hadoop jar ~/hadoop/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.2.jar grep input output '[a-z.]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/hadoop/bin/hdfs dfs -get output output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/hadoop/bin/hdfs dfs -cat output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/Users/wiraramawedashwara/hadoop/hadoop/share/hadoop/common/lib/hadoop-auth-2.10.2.jar) to method sun.security.krb5.Config.getInstance()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/11/09 06:55:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/Users/wiraramawedashwara/hadoop/hadoop/share/hadoop/common/lib/hadoop-auth-2.10.2.jar) to method sun.security.krb5.Config.getInstance()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/11/09 06:55:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!~/hadoop/hadoop/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Mapreduce Python (Nilai 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tadi\t1\n",
      "pagi\t1\n",
      "tidak\t1\n",
      "hujan\t1\n",
      "hujan\t1\n",
      "tetapi\t1\n",
      "mendung\t1\n",
      "kemudian\t1\n",
      "iseng\t1\n",
      "pergi\t1\n",
      "ke\t1\n",
      "luar\t1\n",
      "daerah\t1\n",
      "tanpa\t1\n",
      "bawa\t1\n",
      "jas\t1\n",
      "hujan\t1\n",
      "selamat\t1\n",
      "hujan\t1\n",
      "deras\t1\n"
     ]
    }
   ],
   "source": [
    "!echo tadi pagi tidak hujan hujan tetapi mendung. kemudian iseng pergi ke luar daerah tanpa bawa jas hujan, selamat hujan deras | mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bawa\t1\n",
      "daerah\t1\n",
      "deras\t1\n",
      "hujan\t4\n",
      "iseng\t1\n",
      "jas\t1\n",
      "ke\t1\n",
      "kemudian\t1\n",
      "luar\t1\n",
      "mendung\t1\n",
      "pagi\t1\n",
      "pergi\t1\n",
      "selamat\t1\n",
      "tadi\t1\n",
      "tanpa\t1\n",
      "tetapi\t1\n",
      "tidak\t1\n"
     ]
    }
   ],
   "source": [
    "!echo tadi pagi tidak hujan hujan tetapi mendung. kemudian iseng pergi ke luar daerah tanpa bawa jas hujan, selamat hujan deras | mapper.py | reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/python/bigdata2022/data/pg20417.txt | ~/python/bigdata2022/mapper.py | sort -k1,1 | ~/python/bigdata2022/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ~/python/bigdata2022/data-hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/hadoop/bin/hadoop jar ~/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.10.2.jar -mapper mapper.py -reducer reducer.py -input ~/python/bigdata2022/data/pg20417.txt -output ~/python/bigdata2022/data-hasil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!~/hadoop/hadoop/bin/hadoop jar ~/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.10.2.jar -mapper mapper.py -reducer reducer.py -input ~/python/bigdata2022/data/pg20417.txt -output ~/python/bigdata2022/data-hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/python/bigdata2022/data-hasil/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "untuk mac saja\n",
    "!sudo launchctl load -w /System/Library/LaunchDaemons/ssh.plist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password:\n",
      "sudo: a password is required\n"
     ]
    }
   ],
   "source": [
    "!sudo launchctl unload /System/Library/LaunchDaemons/ssh.plist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/python/bigdata2022/data/articles.csv | ~/python/bigdata2022/mappercsv.py | sort -k1,1 | ~/python/bigdata2022/reducer.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
