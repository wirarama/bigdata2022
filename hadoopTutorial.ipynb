{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konfigurasi stand alone (nilai 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop [--config confdir] [--loglevel loglevel] COMMAND\n",
      "where COMMAND is one of:\n",
      "  fs                   run a generic filesystem user client\n",
      "  version              print the version\n",
      "  jar <jar>            run a jar file\n",
      "                       note: please use \"yarn jar\" to launch\n",
      "                             YARN applications, not this command.\n",
      "  checknative [-a|-h]  check native hadoop and compression libraries availability\n",
      "  distcp <srcurl> <desturl> copy file or directories recursively\n",
      "  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive\n",
      "  classpath            prints the class path needed to get the\n",
      "                       Hadoop jar and the required libraries\n",
      "  credential           interact with credential providers\n",
      "  key                  manage keys via the KeyProvider\n",
      "  daemonlog            get/set the log level for each daemon\n",
      " or\n",
      "  CLASSNAME            run the class named CLASSNAME\n",
      "\n",
      "Most commands print help when invoked w/o parameters.\n"
     ]
    }
   ],
   "source": [
    "!hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop [--config confdir] [--loglevel loglevel] COMMAND\n",
      "where COMMAND is one of:\n",
      "  fs                   run a generic filesystem user client\n",
      "  version              print the version\n",
      "  jar <jar>            run a jar file\n",
      "                       note: please use \"yarn jar\" to launch\n",
      "                             YARN applications, not this command.\n",
      "  checknative [-a|-h]  check native hadoop and compression libraries availability\n",
      "  distcp <srcurl> <desturl> copy file or directories recursively\n",
      "  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive\n",
      "  classpath            prints the class path needed to get the\n",
      "                       Hadoop jar and the required libraries\n",
      "  credential           interact with credential providers\n",
      "  key                  manage keys via the KeyProvider\n",
      "  daemonlog            get/set the log level for each daemon\n",
      " or\n",
      "  CLASSNAME            run the class named CLASSNAME\n",
      "\n",
      "Most commands print help when invoked w/o parameters.\n"
     ]
    }
   ],
   "source": [
    "!C:\\Users\\wirar\\hadoop\\bin\\hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ~/hadoop/hadoop/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ~/hadoop/hadoop/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/hadoop/hadoop/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#untuk windows\n",
    "!cp C:\\Users\\wirar\\hadoop\\etc\\hadoop\\*.xml C:\\Users\\wirar\\hadoop\\etc\\hadoop\\input\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#untuk WSL\n",
    "mkdir $HADOOP_HOME/etc/hadoop/input\n",
    "cp $HADOOP_HOME/etc/hadoop/*.xml $HADOOP_HOME/etc/hadoop/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ~/hadoop/hadoop/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/31 10:43:27 ERROR conf.Configuration: error parsing conf core-site.xml\n",
      "com.ctc.wstx.exc.WstxEOFException: Unexpected EOF in prolog\n",
      " at [row,col,system-id]: [26,0,\"file:/C:/Users/wirar/hadoop/etc/hadoop/core-site.xml\"]\n",
      "\tat com.ctc.wstx.sr.StreamScanner.throwUnexpectedEOF(StreamScanner.java:701)\n",
      "\tat com.ctc.wstx.sr.BasicStreamReader.handleEOF(BasicStreamReader.java:2217)\n",
      "\tat com.ctc.wstx.sr.BasicStreamReader.nextFromProlog(BasicStreamReader.java:2123)\n",
      "\tat com.ctc.wstx.sr.BasicStreamReader.next(BasicStreamReader.java:1179)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2847)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2776)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2654)\n",
      "\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2636)\n",
      "\tat org.apache.hadoop.conf.Configuration.get(Configuration.java:1100)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1707)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1688)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:206)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:302)\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:223)\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:158)\n",
      "Exception in thread \"main\" java.lang.RuntimeException: com.ctc.wstx.exc.WstxEOFException: Unexpected EOF in prolog\n",
      " at [row,col,system-id]: [26,0,\"file:/C:/Users/wirar/hadoop/etc/hadoop/core-site.xml\"]\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3015)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2776)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2654)\n",
      "\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2636)\n",
      "\tat org.apache.hadoop.conf.Configuration.get(Configuration.java:1100)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1707)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1688)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:206)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:302)\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:223)\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:158)\n",
      "Caused by: com.ctc.wstx.exc.WstxEOFException: Unexpected EOF in prolog\n",
      " at [row,col,system-id]: [26,0,\"file:/C:/Users/wirar/hadoop/etc/hadoop/core-site.xml\"]\n",
      "\tat com.ctc.wstx.sr.StreamScanner.throwUnexpectedEOF(StreamScanner.java:701)\n",
      "\tat com.ctc.wstx.sr.BasicStreamReader.handleEOF(BasicStreamReader.java:2217)\n",
      "\tat com.ctc.wstx.sr.BasicStreamReader.nextFromProlog(BasicStreamReader.java:2123)\n",
      "\tat com.ctc.wstx.sr.BasicStreamReader.next(BasicStreamReader.java:1179)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2847)\n",
      "\t... 11 more\n",
      "24/10/31 10:43:27 ERROR conf.Configuration: error parsing conf core-site.xml\n",
      "com.ctc.wstx.exc.WstxEOFException: Unexpected EOF in prolog\n",
      " at [row,col,system-id]: [26,0,\"file:/C:/Users/wirar/hadoop/etc/hadoop/core-site.xml\"]\n",
      "\tat com.ctc.wstx.sr.StreamScanner.throwUnexpectedEOF(StreamScanner.java:701)\n",
      "\tat com.ctc.wstx.sr.BasicStreamReader.handleEOF(BasicStreamReader.java:2217)\n",
      "\tat com.ctc.wstx.sr.BasicStreamReader.nextFromProlog(BasicStreamReader.java:2123)\n",
      "\tat com.ctc.wstx.sr.BasicStreamReader.next(BasicStreamReader.java:1179)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2847)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2776)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2654)\n",
      "\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2636)\n",
      "\tat org.apache.hadoop.conf.Configuration.get(Configuration.java:1100)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1707)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1688)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)\n",
      "Exception in thread \"Thread-0\" java.lang.RuntimeException: com.ctc.wstx.exc.WstxEOFException: Unexpected EOF in prolog\n",
      " at [row,col,system-id]: [26,0,\"file:/C:/Users/wirar/hadoop/etc/hadoop/core-site.xml\"]\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3015)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2776)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2654)\n",
      "\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2636)\n",
      "\tat org.apache.hadoop.conf.Configuration.get(Configuration.java:1100)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1707)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1688)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)\n",
      "Caused by: com.ctc.wstx.exc.WstxEOFException: Unexpected EOF in prolog\n",
      " at [row,col,system-id]: [26,0,\"file:/C:/Users/wirar/hadoop/etc/hadoop/core-site.xml\"]\n",
      "\tat com.ctc.wstx.sr.StreamScanner.throwUnexpectedEOF(StreamScanner.java:701)\n",
      "\tat com.ctc.wstx.sr.BasicStreamReader.handleEOF(BasicStreamReader.java:2217)\n",
      "\tat com.ctc.wstx.sr.BasicStreamReader.nextFromProlog(BasicStreamReader.java:2123)\n",
      "\tat com.ctc.wstx.sr.BasicStreamReader.next(BasicStreamReader.java:1179)\n",
      "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2847)\n",
      "\t... 10 more\n"
     ]
    }
   ],
   "source": [
    "!C:\\Users\\wirar\\hadoop\\bin\\hadoop jar C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-examples-2.10.2.jar grep C:\\Users\\wirar\\hadoop\\input C:\\Users\\wirar\\hadoop\\output '[a-z]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.2.jar grep $HADOOP_HOME/etc/hadoop/input $HADOOP_HOME/etc/hadoop/output '[a-z]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/hadoop/hadoop/output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampai disini bisa nilai :80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konfigurasi HDFS Server (nilai 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!xed ~/hadoop/hadoop/etc/hadoop/core-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://localhost:9000</value>\n",
    "    </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!xed ~/hadoop/hadoop/etc/hadoop/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<configuration>\n",
    "    <property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>1</value>\n",
    "    </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting using clusterid: CID-11a25d3a-7cae-44e3-9311-ef13e2244b7f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/07 10:49:19 INFO namenode.NameNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting NameNode\n",
      "STARTUP_MSG:   host = wirarama/10.80.0.118\n",
      "STARTUP_MSG:   args = [-format]\n",
      "STARTUP_MSG:   version = 2.10.2\n",
      "STARTUP_MSG:   classpath = C:\\Users\\wirar\\hadoop\\etc\\hadoop;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\activation-1.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\apacheds-i18n-2.0.0-M15.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\apacheds-kerberos-codec-2.0.0-M15.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\api-asn1-api-1.0.0-M20.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\api-util-1.0.0-M20.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\asm-3.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\audience-annotations-0.5.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\avro-1.7.7.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-beanutils-1.9.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-cli-1.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-codec-1.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-collections-3.2.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-compress-1.21.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-configuration-1.6.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-digester-1.8.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-io-2.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-lang-2.6.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-lang3-3.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-logging-1.1.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-math3-3.1.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\commons-net-3.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\curator-client-2.13.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\curator-framework-2.13.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\curator-recipes-2.13.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\gson-2.2.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\guava-11.0.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\hadoop-annotations-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\hadoop-auth-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\hamcrest-core-1.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\htrace-core4-4.1.0-incubating.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\httpclient-4.5.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\httpcore-4.4.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jackson-core-asl-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jackson-jaxrs-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jackson-mapper-asl-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jackson-xc-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\java-xmlbuilder-0.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jaxb-api-2.2.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jaxb-impl-2.2.3-1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jcip-annotations-1.0-1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jersey-core-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jersey-json-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jersey-server-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jets3t-0.9.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jettison-1.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jetty-6.1.26.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jetty-sslengine-6.1.26.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jetty-util-6.1.26.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jsch-0.1.55.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\json-smart-1.3.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jsp-api-2.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\jsr305-3.0.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\junit-4.13.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\mockito-all-1.8.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\netty-3.10.6.Final.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\nimbus-jose-jwt-7.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\paranamer-2.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\protobuf-java-2.5.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\reload4j-1.2.18.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\servlet-api-2.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\slf4j-api-1.7.36.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\slf4j-reload4j-1.7.36.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\snappy-java-1.0.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\spotbugs-annotations-3.1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\stax-api-1.0-2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\stax2-api-4.2.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\woodstox-core-5.3.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\xmlenc-0.52.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\lib\\zookeeper-3.4.14.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\hadoop-common-2.10.2-tests.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\hadoop-common-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\common\\hadoop-nfs-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\asm-3.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\commons-cli-1.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\commons-codec-1.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\commons-daemon-1.0.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\commons-io-2.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\commons-lang-2.6.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\commons-logging-1.1.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\guava-11.0.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\hadoop-hdfs-client-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\htrace-core4-4.1.0-incubating.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jackson-annotations-2.9.10.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jackson-core-2.9.10.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jackson-core-asl-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jackson-databind-2.9.10.7.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jackson-mapper-asl-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jersey-core-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jersey-server-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jetty-6.1.26.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jetty-util-6.1.26.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\jsr305-3.0.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\leveldbjni-all-1.8.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\netty-3.10.6.Final.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\netty-all-4.1.50.Final.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\okhttp-2.7.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\okio-1.6.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\protobuf-java-2.5.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\reload4j-1.2.18.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\servlet-api-2.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\xercesImpl-2.12.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\xml-apis-1.4.01.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\lib\\xmlenc-0.52.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-2.10.2-tests.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-client-2.10.2-tests.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-client-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-native-client-2.10.2-tests.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-native-client-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-nfs-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-rbf-2.10.2-tests.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\hdfs\\hadoop-hdfs-rbf-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\activation-1.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\aopalliance-1.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\apacheds-i18n-2.0.0-M15.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\apacheds-kerberos-codec-2.0.0-M15.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\api-asn1-api-1.0.0-M20.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\api-util-1.0.0-M20.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\asm-3.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\audience-annotations-0.5.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\avro-1.7.7.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-beanutils-1.9.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-cli-1.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-codec-1.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-collections-3.2.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-compress-1.21.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-configuration-1.6.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-digester-1.8.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-io-2.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-lang-2.6.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-lang3-3.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-logging-1.1.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-math3-3.1.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\commons-net-3.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\curator-client-2.13.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\curator-framework-2.13.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\curator-recipes-2.13.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\ehcache-3.3.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\fst-2.50.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\geronimo-jcache_1.0_spec-1.0-alpha-1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\gson-2.2.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\guava-11.0.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\guice-3.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\guice-servlet-3.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\HikariCP-java7-2.4.12.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\htrace-core4-4.1.0-incubating.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\httpclient-4.5.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\httpcore-4.4.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jackson-core-asl-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jackson-jaxrs-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jackson-mapper-asl-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jackson-xc-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\java-util-1.9.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\java-xmlbuilder-0.4.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\javax.inject-1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jaxb-api-2.2.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jaxb-impl-2.2.3-1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jcip-annotations-1.0-1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jersey-client-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jersey-core-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jersey-guice-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jersey-json-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jersey-server-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jets3t-0.9.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jettison-1.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jetty-6.1.26.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jetty-sslengine-6.1.26.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jetty-util-6.1.26.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jsch-0.1.55.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\json-io-2.5.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\json-smart-1.3.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jsp-api-2.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\jsr305-3.0.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\leveldbjni-all-1.8.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\metrics-core-3.0.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\mssql-jdbc-6.2.1.jre7.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\netty-3.10.6.Final.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\nimbus-jose-jwt-7.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\paranamer-2.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\protobuf-java-2.5.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\reload4j-1.2.18.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\servlet-api-2.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\snappy-java-1.0.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\spotbugs-annotations-3.1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\stax-api-1.0-2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\stax2-api-4.2.1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\woodstox-core-5.3.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\xmlenc-0.52.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\lib\\zookeeper-3.4.14.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-api-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-applications-distributedshell-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-applications-unmanaged-am-launcher-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-client-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-common-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-registry-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-applicationhistoryservice-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-common-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-nodemanager-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-resourcemanager-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-router-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-sharedcachemanager-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-tests-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-timeline-pluginstorage-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\yarn\\hadoop-yarn-server-web-proxy-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\aopalliance-1.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\asm-3.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\avro-1.7.7.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\commons-compress-1.21.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\commons-io-2.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\guice-3.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\guice-servlet-3.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\hadoop-annotations-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\hamcrest-core-1.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\jackson-core-asl-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\jackson-mapper-asl-1.9.13.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\javax.inject-1.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\jersey-core-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\jersey-guice-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\jersey-server-1.9.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\junit-4.13.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\leveldbjni-all-1.8.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\netty-3.10.6.Final.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\paranamer-2.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\protobuf-java-2.5.0.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\reload4j-1.2.18.3.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\lib\\snappy-java-1.0.5.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-client-app-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-client-common-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-client-core-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-client-hs-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-client-hs-plugins-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-client-jobclient-2.10.2-tests.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-client-jobclient-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-client-shuffle-2.10.2.jar;C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-examples-2.10.2.jar\n",
      "STARTUP_MSG:   build = Unknown -r 965fd380006fa78b2315668fbc7eb432e1d8200f; compiled by 'ubuntu' on 2022-05-24T22:35Z\n",
      "STARTUP_MSG:   java = 17.0.9\n",
      "************************************************************/\n",
      "24/11/07 10:49:19 INFO namenode.NameNode: createNameNode [-format]\n",
      "24/11/07 10:49:21 INFO namenode.FSEditLog: Edit logging is async:true\n",
      "24/11/07 10:49:21 INFO namenode.FSNamesystem: KeyProvider: null\n",
      "24/11/07 10:49:21 INFO namenode.FSNamesystem: fsLock is fair: true\n",
      "24/11/07 10:49:21 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
      "24/11/07 10:49:21 INFO namenode.FSNamesystem: fsOwner             = wirar (auth:SIMPLE)\n",
      "24/11/07 10:49:21 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
      "24/11/07 10:49:21 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
      "24/11/07 10:49:21 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "24/11/07 10:49:21 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "24/11/07 10:49:21 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
      "24/11/07 10:49:21 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "24/11/07 10:49:21 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "24/11/07 10:49:21 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Nov 07 10:49:21\n",
      "24/11/07 10:49:21 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "24/11/07 10:49:21 INFO util.GSet: VM type       = 64-bit\n",
      "24/11/07 10:49:21 INFO util.GSet: 2.0% max memory 1000 MB = 20 MB\n",
      "24/11/07 10:49:21 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n",
      "24/11/07 10:49:21 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\n",
      "24/11/07 10:49:21 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS\n",
      "24/11/07 10:49:21 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\n",
      "24/11/07 10:49:21 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
      "24/11/07 10:49:21 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
      "24/11/07 10:49:21 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
      "24/11/07 10:49:21 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
      "24/11/07 10:49:21 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "24/11/07 10:49:21 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "24/11/07 10:49:21 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "24/11/07 10:49:21 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\n",
      "24/11/07 10:49:21 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "24/11/07 10:49:21 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "24/11/07 10:49:21 INFO namenode.FSNamesystem: Append Enabled: true\n",
      "24/11/07 10:49:21 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215\n",
      "24/11/07 10:49:21 INFO util.GSet: Computing capacity for map INodeMap\n",
      "24/11/07 10:49:21 INFO util.GSet: VM type       = 64-bit\n",
      "24/11/07 10:49:21 INFO util.GSet: 1.0% max memory 1000 MB = 10 MB\n",
      "24/11/07 10:49:21 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
      "24/11/07 10:49:21 INFO namenode.FSDirectory: ACLs enabled? false\n",
      "24/11/07 10:49:21 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "24/11/07 10:49:21 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
      "24/11/07 10:49:21 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false\n",
      "24/11/07 10:49:21 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "24/11/07 10:49:21 INFO util.GSet: VM type       = 64-bit\n",
      "24/11/07 10:49:21 INFO util.GSet: 0.25% max memory 1000 MB = 2.5 MB\n",
      "24/11/07 10:49:21 INFO util.GSet: capacity      = 2^18 = 262144 entries\n",
      "24/11/07 10:49:21 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "24/11/07 10:49:21 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "24/11/07 10:49:21 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "24/11/07 10:49:21 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "24/11/07 10:49:21 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "24/11/07 10:49:21 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "24/11/07 10:49:21 INFO util.GSet: VM type       = 64-bit\n",
      "24/11/07 10:49:21 INFO util.GSet: 0.029999999329447746% max memory 1000 MB = 307.2 KB\n",
      "24/11/07 10:49:21 INFO util.GSet: capacity      = 2^15 = 32768 entries\n",
      "24/11/07 10:49:21 INFO namenode.FSImage: Allocated new BlockPoolId: BP-800580523-10.80.0.118-1730947761868\n",
      "24/11/07 10:49:21 INFO common.Storage: Storage directory \\tmp\\hadoop-wirar\\dfs\\name has been successfully formatted.\n",
      "24/11/07 10:49:21 INFO namenode.FSImageFormatProtobuf: Saving image file \\tmp\\hadoop-wirar\\dfs\\name\\current\\fsimage.ckpt_0000000000000000000 using no compression\n",
      "24/11/07 10:49:22 INFO namenode.FSImageFormatProtobuf: Image file \\tmp\\hadoop-wirar\\dfs\\name\\current\\fsimage.ckpt_0000000000000000000 of size 324 bytes saved in 0 seconds .\n",
      "24/11/07 10:49:22 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
      "24/11/07 10:49:22 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid = 0 when meet shutdown.\n",
      "24/11/07 10:49:22 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down NameNode at wirarama/10.80.0.118\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "!C:\\Users\\wirar\\hadoop\\bin\\hdfs namenode -format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!C:\\Users\\wirar\\hadoop\\sbin\\start-dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:50070/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/wirar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 wirar supergroup          0 2024-11-07 10:55 /user/wirar/output/_SUCCESS\n",
      "-rw-r--r--   1 wirar supergroup          0 2024-11-07 10:55 /user/wirar/output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/wirar/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-rm: Illegal option -rf\n",
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] [-x] <path> ...]\n",
      "\t[-expunge]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] <file>]\n",
      "\t[-test -[defsz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are:\n",
      "-conf <configuration file>        specify an application configuration file\n",
      "-D <property=value>               define a value for a given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
      "\n",
      "The general command line syntax is:\n",
      "command [genericOptions] [commandOptions]\n",
      "\n",
      "Usage: hadoop fs [generic options] -rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /user/wirar/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/Users/wiraramawedashwara/hadoop/hadoop/share/hadoop/common/lib/hadoop-auth-2.10.2.jar) to method sun.security.krb5.Config.getInstance()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/11/09 06:54:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/wiraramawedashwara/httpdata/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/hadoop/bin/hdfs dfs -put ~/hadoop/hadoop/etc/hadoop input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put C:\\Users\\wirar\\hadoop\\input\\pg20417.txt /user/wirar/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/hadoop/bin/hadoop jar ~/hadoop/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.2.jar grep input output '[a-z.]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/07 11:01:58 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "24/11/07 11:01:58 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "24/11/07 11:01:58 INFO input.FileInputFormat: Total input files to process : 1\n",
      "24/11/07 11:01:59 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "24/11/07 11:01:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2041364980_0001\n",
      "24/11/07 11:01:59 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "24/11/07 11:01:59 INFO mapreduce.Job: Running job: job_local2041364980_0001\n",
      "24/11/07 11:01:59 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "24/11/07 11:01:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/11/07 11:01:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/11/07 11:01:59 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "24/11/07 11:01:59 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "24/11/07 11:01:59 INFO mapred.LocalJobRunner: Starting task: attempt_local2041364980_0001_m_000000_0\n",
      "24/11/07 11:01:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/11/07 11:01:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/11/07 11:01:59 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "24/11/07 11:01:59 INFO mapred.Task:  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@2ed78d48\n",
      "24/11/07 11:01:59 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/wirar/input:0+661807\n",
      "24/11/07 11:01:59 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "24/11/07 11:01:59 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "24/11/07 11:01:59 INFO mapred.MapTask: soft limit at 83886080\n",
      "24/11/07 11:01:59 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "24/11/07 11:01:59 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "24/11/07 11:01:59 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "24/11/07 11:01:59 INFO mapred.LocalJobRunner: \n",
      "24/11/07 11:01:59 INFO mapred.MapTask: Starting flush of map output\n",
      "24/11/07 11:01:59 INFO mapred.Task: Task:attempt_local2041364980_0001_m_000000_0 is done. And is in the process of committing\n",
      "24/11/07 11:01:59 INFO mapred.LocalJobRunner: map\n",
      "24/11/07 11:01:59 INFO mapred.Task: Task 'attempt_local2041364980_0001_m_000000_0' done.\n",
      "24/11/07 11:01:59 INFO mapred.Task: Final Counters for attempt_local2041364980_0001_m_000000_0: Counters: 23\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=303473\n",
      "\t\tFILE: Number of bytes written=803212\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=661807\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=5\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=1\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12760\n",
      "\t\tMap output records=0\n",
      "\t\tMap output bytes=0\n",
      "\t\tMap output materialized bytes=6\n",
      "\t\tInput split bytes=103\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=168820736\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=661807\n",
      "24/11/07 11:01:59 INFO mapred.LocalJobRunner: Finishing task: attempt_local2041364980_0001_m_000000_0\n",
      "24/11/07 11:01:59 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "24/11/07 11:01:59 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "24/11/07 11:01:59 INFO mapred.LocalJobRunner: Starting task: attempt_local2041364980_0001_r_000000_0\n",
      "24/11/07 11:01:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/11/07 11:01:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/11/07 11:01:59 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "24/11/07 11:01:59 INFO mapred.Task:  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@4cf1ee9b\n",
      "24/11/07 11:01:59 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@34df0870\n",
      "24/11/07 11:01:59 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=375809632, maxSingleShuffleLimit=93952408, mergeThreshold=248034368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "24/11/07 11:01:59 INFO reduce.EventFetcher: attempt_local2041364980_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "24/11/07 11:01:59 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2041364980_0001_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "24/11/07 11:01:59 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local2041364980_0001_m_000000_0\n",
      "24/11/07 11:01:59 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "24/11/07 11:01:59 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "24/11/07 11:01:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "24/11/07 11:01:59 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "24/11/07 11:01:59 INFO mapred.Merger: Merging 1 sorted segments\n",
      "24/11/07 11:01:59 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "24/11/07 11:01:59 INFO reduce.MergeManagerImpl: Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit\n",
      "24/11/07 11:01:59 INFO reduce.MergeManagerImpl: Merging 1 files, 6 bytes from disk\n",
      "24/11/07 11:01:59 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "24/11/07 11:01:59 INFO mapred.Merger: Merging 1 sorted segments\n",
      "24/11/07 11:01:59 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "24/11/07 11:01:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "24/11/07 11:02:00 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "24/11/07 11:02:00 INFO mapred.Task: Task:attempt_local2041364980_0001_r_000000_0 is done. And is in the process of committing\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "24/11/07 11:02:00 INFO mapred.Task: Task attempt_local2041364980_0001_r_000000_0 is allowed to commit now\n",
      "24/11/07 11:02:00 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2041364980_0001_r_000000_0' to hdfs://localhost:9000/user/wirar/grep-temp-1941334442/_temporary/0/task_local2041364980_0001_r_000000\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "24/11/07 11:02:00 INFO mapred.Task: Task 'attempt_local2041364980_0001_r_000000_0' done.\n",
      "24/11/07 11:02:00 INFO mapred.Task: Final Counters for attempt_local2041364980_0001_r_000000_0: Counters: 29\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=303517\n",
      "\t\tFILE: Number of bytes written=803218\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=661807\n",
      "\t\tHDFS: Number of bytes written=86\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=0\n",
      "\t\tReduce shuffle bytes=6\n",
      "\t\tReduce input records=0\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=168820736\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=86\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: Finishing task: attempt_local2041364980_0001_r_000000_0\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "24/11/07 11:02:00 INFO mapreduce.Job: Job job_local2041364980_0001 running in uber mode : false\n",
      "24/11/07 11:02:00 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "24/11/07 11:02:00 INFO mapreduce.Job: Job job_local2041364980_0001 completed successfully\n",
      "24/11/07 11:02:00 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=606990\n",
      "\t\tFILE: Number of bytes written=1606430\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1323614\n",
      "\t\tHDFS: Number of bytes written=86\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12760\n",
      "\t\tMap output records=0\n",
      "\t\tMap output bytes=0\n",
      "\t\tMap output materialized bytes=6\n",
      "\t\tInput split bytes=103\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=0\n",
      "\t\tReduce shuffle bytes=6\n",
      "\t\tReduce input records=0\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=337641472\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=661807\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=86\n",
      "24/11/07 11:02:00 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "24/11/07 11:02:00 INFO input.FileInputFormat: Total input files to process : 1\n",
      "24/11/07 11:02:00 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "24/11/07 11:02:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local560902005_0002\n",
      "24/11/07 11:02:00 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "24/11/07 11:02:00 INFO mapreduce.Job: Running job: job_local560902005_0002\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "24/11/07 11:02:00 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/11/07 11:02:00 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: Starting task: attempt_local560902005_0002_m_000000_0\n",
      "24/11/07 11:02:00 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/11/07 11:02:00 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/11/07 11:02:00 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "24/11/07 11:02:00 INFO mapred.Task:  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@67bf4c2c\n",
      "24/11/07 11:02:00 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/wirar/grep-temp-1941334442/part-r-00000:0+86\n",
      "24/11/07 11:02:00 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "24/11/07 11:02:00 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "24/11/07 11:02:00 INFO mapred.MapTask: soft limit at 83886080\n",
      "24/11/07 11:02:00 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "24/11/07 11:02:00 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "24/11/07 11:02:00 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: \n",
      "24/11/07 11:02:00 INFO mapred.MapTask: Starting flush of map output\n",
      "24/11/07 11:02:00 INFO mapred.Task: Task:attempt_local560902005_0002_m_000000_0 is done. And is in the process of committing\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: map\n",
      "24/11/07 11:02:00 INFO mapred.Task: Task 'attempt_local560902005_0002_m_000000_0' done.\n",
      "24/11/07 11:02:00 INFO mapred.Task: Final Counters for attempt_local560902005_0002_m_000000_0: Counters: 22\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=607015\n",
      "\t\tFILE: Number of bytes written=1602205\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=661893\n",
      "\t\tHDFS: Number of bytes written=86\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=7\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=0\n",
      "\t\tMap output records=0\n",
      "\t\tMap output bytes=0\n",
      "\t\tMap output materialized bytes=6\n",
      "\t\tInput split bytes=131\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=1\n",
      "\t\tTotal committed heap usage (bytes)=168820736\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=86\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: Finishing task: attempt_local560902005_0002_m_000000_0\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: Starting task: attempt_local560902005_0002_r_000000_0\n",
      "24/11/07 11:02:00 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/11/07 11:02:00 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/11/07 11:02:00 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "24/11/07 11:02:00 INFO mapred.Task:  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@19af0858\n",
      "24/11/07 11:02:00 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1d908fb5\n",
      "24/11/07 11:02:00 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=375809632, maxSingleShuffleLimit=93952408, mergeThreshold=248034368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "24/11/07 11:02:00 INFO reduce.EventFetcher: attempt_local560902005_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "24/11/07 11:02:00 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local560902005_0002_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "24/11/07 11:02:00 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local560902005_0002_m_000000_0\n",
      "24/11/07 11:02:00 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "24/11/07 11:02:00 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "24/11/07 11:02:00 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "24/11/07 11:02:00 INFO mapred.Merger: Merging 1 sorted segments\n",
      "24/11/07 11:02:00 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "24/11/07 11:02:00 INFO reduce.MergeManagerImpl: Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit\n",
      "24/11/07 11:02:00 INFO reduce.MergeManagerImpl: Merging 1 files, 6 bytes from disk\n",
      "24/11/07 11:02:00 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "24/11/07 11:02:00 INFO mapred.Merger: Merging 1 sorted segments\n",
      "24/11/07 11:02:00 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "24/11/07 11:02:00 INFO mapred.Task: Task:attempt_local560902005_0002_r_000000_0 is done. And is in the process of committing\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "24/11/07 11:02:00 INFO mapred.Task: Task attempt_local560902005_0002_r_000000_0 is allowed to commit now\n",
      "24/11/07 11:02:00 INFO output.FileOutputCommitter: Saved output of task 'attempt_local560902005_0002_r_000000_0' to hdfs://localhost:9000/user/wirar/output1/_temporary/0/task_local560902005_0002_r_000000\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "24/11/07 11:02:00 INFO mapred.Task: Task 'attempt_local560902005_0002_r_000000_0' done.\n",
      "24/11/07 11:02:00 INFO mapred.Task: Final Counters for attempt_local560902005_0002_r_000000_0: Counters: 29\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=607059\n",
      "\t\tFILE: Number of bytes written=1602211\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=661893\n",
      "\t\tHDFS: Number of bytes written=86\n",
      "\t\tHDFS: Number of read operations=21\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=0\n",
      "\t\tReduce shuffle bytes=6\n",
      "\t\tReduce input records=0\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=168820736\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: Finishing task: attempt_local560902005_0002_r_000000_0\n",
      "24/11/07 11:02:00 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "24/11/07 11:02:01 INFO mapreduce.Job: Job job_local560902005_0002 running in uber mode : false\n",
      "24/11/07 11:02:01 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "24/11/07 11:02:01 INFO mapreduce.Job: Job job_local560902005_0002 completed successfully\n",
      "24/11/07 11:02:01 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1214074\n",
      "\t\tFILE: Number of bytes written=3204416\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1323786\n",
      "\t\tHDFS: Number of bytes written=172\n",
      "\t\tHDFS: Number of read operations=39\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=0\n",
      "\t\tMap output records=0\n",
      "\t\tMap output bytes=0\n",
      "\t\tMap output materialized bytes=6\n",
      "\t\tInput split bytes=131\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=0\n",
      "\t\tReduce shuffle bytes=6\n",
      "\t\tReduce input records=0\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=1\n",
      "\t\tTotal committed heap usage (bytes)=337641472\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=86\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar C:\\Users\\wirar\\hadoop\\share\\hadoop\\mapreduce\\hadoop-mapreduce-examples-2.10.2.jar grep input output1 '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get: `output/output/_SUCCESS': File exists\n",
      "get: `output/output/part-r-00000': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -get output output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat output1/part-r-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO: No tasks running with the specified criteria.\n",
      "\n",
      "INFO: No tasks running with the specified criteria.\n"
     ]
    }
   ],
   "source": [
    "!C:\\Users\\wirar\\hadoop\\sbin\\stop-dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Mapreduce Python (Nilai 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tadi\t1\n",
      "pagi\t1\n",
      "tidak\t1\n",
      "hujan\t1\n",
      "hujan\t1\n",
      "tetapi\t1\n",
      "mendung\t1\n",
      "kemudian\t1\n",
      "iseng\t1\n",
      "pergi\t1\n",
      "ke\t1\n",
      "luar\t1\n",
      "daerah\t1\n",
      "tanpa\t1\n",
      "bawa\t1\n",
      "jas\t1\n",
      "hujan\t1\n",
      "selamat\t1\n",
      "hujan\t1\n",
      "deras\t1\n"
     ]
    }
   ],
   "source": [
    "!echo tadi pagi tidak hujan hujan tetapi mendung. kemudian iseng pergi ke luar daerah tanpa bawa jas hujan, selamat hujan deras | python mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bawa\t1\n",
      "daerah\t1\n",
      "deras\t1\n",
      "hujan\t4\n",
      "iseng\t1\n",
      "jas\t1\n",
      "ke\t1\n",
      "kemudian\t1\n",
      "luar\t1\n",
      "mendung\t1\n",
      "pagi\t1\n",
      "pergi\t1\n",
      "selamat\t1\n",
      "tadi\t1\n",
      "tanpa\t1\n",
      "tetapi\t1\n",
      "tidak\t1\n"
     ]
    }
   ],
   "source": [
    "!echo tadi pagi tidak hujan hujan tetapi mendung. kemudian iseng pergi ke luar daerah tanpa bawa jas hujan, selamat hujan deras | python mapper.py | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!cat pg20417.txt | python mapper.py | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ~/python/bigdata2022/data-hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/hadoop/hadoop/bin/hadoop jar ~/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.10.2.jar -mapper mapper.py -reducer reducer.py -input ~/python/bigdata2022/data/pg20417.txt -output ~/python/bigdata2022/data-hasil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!~/hadoop/hadoop/bin/hadoop jar ~/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.10.2.jar -mapper mapper.py -reducer reducer.py -input ~/python/bigdata2022/data/pg20417.txt -output ~/python/bigdata2022/data-hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/python/bigdata2022/data-hasil/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "untuk mac saja\n",
    "!sudo launchctl load -w /System/Library/LaunchDaemons/ssh.plist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password:\n",
      "sudo: a password is required\n"
     ]
    }
   ],
   "source": [
    "!sudo launchctl unload /System/Library/LaunchDaemons/ssh.plist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/python/bigdata2022/data/articles.csv | ~/python/bigdata2022/mappercsv.py | sort -k1,1 | ~/python/bigdata2022/reducer.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
